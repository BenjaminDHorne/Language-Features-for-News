import codecsimport collectionsimport osimport stringimport unicodedatafrom multiprocessing import Processfrom pathlib import Pathimport numpy as npimport nltkfrom nltk import tokenize, word_tokenizefrom nltk.corpus import stopwordsfrom nltk.stem.porter import *from nltk.util import ngramsfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzerfrom compute_feats_fast_py3.src.readability import Readabilityfrom compute_feats_fast_py3.src.tokenizing import NewTokenizerbase_path = Path.cwd().resolve()if "compute_feats_fast_py3" not in str(base_path):    base_path = Path(base_path, "compute_feats_fast_py3")utility_data_path = str(Path(base_path, "utility_data"))stopwords = set(stopwords.words('english'))# Feature Functions <-----------------------------------------------------------------------def load_happiness_index_lexicon(filepath=utility_data_path):    word_to_happiness = {}    with open(os.path.join(filepath, "happiness_index.txt")) as lex:        lex.readline()        for line in lex:            line = line.strip().split("\t")            word_to_happiness[line[0]] = line[2]    return word_to_happinessdef happiness_index_feats(happiness, text, feature_dictionary):    happiness_scores = []    tokens = word_tokenize(text)    tokens = [t.lower() for t in tokens]    for token in tokens:        if token not in stopwords:            if token in happiness.keys():                happiness_scores.append(float(happiness[token]))            else:                happiness_scores.append(5)    if len(happiness_scores) == 0:        return 0    h = float(sum(happiness_scores)) / len(happiness_scores)    feature_dictionary['Happiness'] = hdef load_moral_foundations_lexicon(filepath=utility_data_path):    code_to_foundation = {}    foundation_to_lex = {}    with open(os.path.join(filepath, "moral foundations dictionary.dic")) as lex:        header_token = fix(lex.readline())        for line in lex:            line = fix(line)            if line == header_token:                break            code_foundation = line.strip().split()            code_to_foundation[code_foundation[0]] = code_foundation[1]            foundation_to_lex[code_foundation[1]] = []        for line in lex:            try:                word_code = line.strip().split()                stem = word_code[0].replace("*", "")                codes = word_code[1:]                for x in range(len(codes)):                    foundation_to_lex[code_to_foundation[codes[x]]].append(stem)            except:                continue    return foundation_to_lexdef moral_foundation_feats(foundation_lex_dictionary, text, feature_dictionary):    foundation_counts = {}    tokens = word_tokenize(text)    stemmer = PorterStemmer()    stemed_tokens = [stemmer.stem(t) for t in tokens]    for key in foundation_lex_dictionary.keys():        foundation_counts[key] = float(sum([stemed_tokens.count(i) for i in foundation_lex_dictionary[key]])) / len(            stemed_tokens)    feature_dictionary["HarmVirtue"] = foundation_counts["HarmVirtue"]    feature_dictionary["HarmVice"] = foundation_counts["HarmVice"]    feature_dictionary["FairnessVirtue"] = foundation_counts["FairnessVirtue"]    feature_dictionary["FairnessVice"] = foundation_counts["FairnessVice"]    feature_dictionary["IngroupVirtue"] = foundation_counts["IngroupVirtue"]    feature_dictionary["IngroupVice"] = foundation_counts["IngroupVice"]    feature_dictionary["AuthorityVirtue"] = foundation_counts["AuthorityVirtue"]    feature_dictionary["AuthorityVice"] = foundation_counts["AuthorityVice"]    feature_dictionary["PurityVirtue"] = foundation_counts["PurityVirtue"]    feature_dictionary["PurityVice"] = foundation_counts["PurityVice"]    feature_dictionary["MoralityGeneral"] = foundation_counts["MoralityGeneral"]def load_acl13_lexicons(filepath=utility_data_path):    with open(os.path.join(filepath, "bias-lexicon.txt")) as lex:        bias = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "assertives.txt")) as lex:        assertives = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "factives.txt")) as lex:        factives = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "hedges.txt")) as lex:        hedges = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "implicatives.txt")) as lex:        implicatives = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "report_verbs.txt")) as lex:        report_verbs = set([fix(l.strip()) for l in lex])    with codecs.open(os.path.join(filepath, "negative-words.txt"), encoding="utf-8", errors="ignore") as lex:        negative = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "positive-words.txt")) as lex:        positive = set([fix(l.strip()) for l in lex])    with open(os.path.join(filepath, "subjclueslen.txt")) as lex:        wneg = set([])        wpos = set([])        wneu = set([])        sneg = set([])        spos = set([])        sneu = set([])        for line in lex:            line = fix(line).split()            if line[0] == "type=weaksubj":                if line[-1] == "priorpolarity=negative":                    wneg.add(line[2].split("=")[1])                elif line[-1] == "priorpolarity=positive":                    wpos.add(line[2].split("=")[1])                elif line[-1] == "priorpolarity=neutral":                    wneu.add(line[2].split("=")[1])                elif line[-1] == "priorpolarity=both":                    wneg.add(line[2].split("=")[1])                    wpos.add(line[2].split("=")[1])            elif line[0] == "type=strongsubj":                if line[-1] == "priorpolarity=negative":                    sneg.add(line[2].split("=")[1])                elif line[-1] == "priorpolarity=positive":                    spos.add(line[2].split("=")[1])                elif line[-1] == "priorpolarity=neutral":                    sneu.add(line[2].split("=")[1])                elif line[-1] == "priorpolarity=both":                    spos.add(line[2].split("=")[1])                    sneg.add(line[2].split("=")[1])    return bias, assertives, factives, hedges, implicatives, report_verbs, positive, negative, wneg, wpos, wneu, sneg, \           spos, sneudef bias_lexicon_feats(bias, assertives, factives, hedges, implicatives, report_verbs, positive_op, negative_op, wneg,                       wpos, wneu, sneg, spos, sneu, text, feature_dictionary, tokenizer: NewTokenizer):    do_check = False    # tokens = word_tokenize(text)    tokens = tokenizer.word_tokenize(text=text)    bigrams = [" ".join(bg) for bg in ngrams(tokens, 2)]    trigrams = [" ".join(tg) for tg in ngrams(tokens, 3)]    hedges_count = sum([tokens.count(h) for h in hedges]) + sum([bigrams.count(h) for h in hedges]) + sum(        [trigrams.count(h) for h in hedges])    hedges_count = float(hedges_count) / len(tokens)    bias_count_alt = 0.    assertives_count_alt = 0.    factives_count_alt = 0.    implicatives_count_alt = 0.    report_verbs_count_alt = 0.    positive_op_count_alt = 0.    negative_op_count_alt = 0.    wneg_count_alt = 0.    wpos_count_alt = 0.    wneu_count_alt = 0.    sneg_count_alt = 0.    spos_count_alt = 0.    sneu_count_alt = 0.    incr = 1. / len(tokens)    for val in tokens:        if val in bias:            bias_count_alt += incr        if val in assertives:            assertives_count_alt += incr        if val in factives:            factives_count_alt += incr        if val in implicatives:            implicatives_count_alt += incr        if val in report_verbs:            report_verbs_count_alt += incr        if val in positive_op:            positive_op_count_alt += incr        if val in negative_op:            negative_op_count_alt += incr        if val in wneg:            wneg_count_alt += incr        if val in wpos:            wpos_count_alt += incr        if val in wneu:            wneu_count_alt += incr        if val in sneg:            sneg_count_alt += incr        if val in spos:            spos_count_alt += incr        if val in sneu:            sneu_count_alt += incr    if do_check:        bias_count = float(sum([tokens.count(b) for b in bias])) / len(tokens)        assertives_count = float(sum([tokens.count(a) for a in assertives])) / len(tokens)        factives_count = float(sum([tokens.count(f) for f in factives])) / len(tokens)        implicatives_count = float(sum([tokens.count(i) for i in implicatives])) / len(tokens)        report_verbs_count = float(sum([tokens.count(r) for r in report_verbs])) / len(tokens)        positive_op_count = float(sum([tokens.count(p) for p in positive_op])) / len(tokens)        negative_op_count = float(sum([tokens.count(n) for n in negative_op])) / len(tokens)        wneg_count = float(sum([tokens.count(n) for n in wneg])) / len(tokens)        wpos_count = float(sum([tokens.count(n) for n in wpos])) / len(tokens)        wneu_count = float(sum([tokens.count(n) for n in wneu])) / len(tokens)        sneg_count = float(sum([tokens.count(n) for n in sneg])) / len(tokens)        spos_count = float(sum([tokens.count(n) for n in spos])) / len(tokens)        sneu_count = float(sum([tokens.count(n) for n in sneu])) / len(tokens)        t1 = np.array([bias_count_alt, assertives_count_alt, factives_count_alt, implicatives_count_alt,                       report_verbs_count_alt, positive_op_count_alt, negative_op_count_alt,                       wneg_count_alt, wpos_count_alt, wneu_count_alt, sneg_count_alt, spos_count_alt, sneu_count_alt])        t2 = np.array([bias_count, assertives_count, factives_count, implicatives_count, report_verbs_count,                       positive_op_count, negative_op_count,                       wneg_count, wpos_count, wneu_count, sneg_count, spos_count, sneu_count])        assert np.isclose(t1, t2).all()    feature_dictionary["bias_count"] = bias_count_alt    feature_dictionary["assertives_count"] = assertives_count_alt    feature_dictionary["factives_count"] = factives_count_alt    feature_dictionary["hedges_count"] = hedges_count    feature_dictionary["implicatives_count"] = implicatives_count_alt    feature_dictionary["report_verbs_count"] = report_verbs_count_alt    feature_dictionary["positive_op_count"] = positive_op_count_alt    feature_dictionary["negative_op_count"] = negative_op_count_alt    feature_dictionary["wneg_count"] = wneg_count_alt    feature_dictionary["wpos_count"] = wpos_count_alt    feature_dictionary["wneu_count"] = wneu_count_alt    feature_dictionary["sneg_count"] = sneg_count_alt    feature_dictionary["spos_count"] = spos_count_alt    feature_dictionary["sneu_count"] = sneu_count_altdef ttr(text, feature_dictionary):    words = text.split()    dif_words = len(set(words))    tot_words = len(words)    if tot_words == 0:        feature_dictionary['ttr'] = 0    feature_dictionary['TTR'] = str(float(dif_words) / tot_words)def POS_features(fn, text, outpath, feature_dictionary):    fname = os.path.join(outpath, fn.split(".")[0] + "_tagged.txt")    pos_tags = ["CC", "CD", "DT", "EX", "FW", "IN", "JJ", "JJR", "JJS", "LS", "MD", "NN", "NNS", "NNP", "NNPS", "PDT",                "POS", "PRP", "PRP$", "RB", "RBR", "RBS", "RP", "SYM", "TO", "UH", "WP$", "WRB", "VB", "VBD", "VBG",                "VBN", "VBP", "VBZ", "WDT", "WP"]    sents = tokenize.sent_tokenize(text)    counts_norm = []    allwords = []    sents = tokenize.sent_tokenize(text)    tags = None    with open(fname, "w") as out:        for sent in sents:            words = sent.strip(".").split()            tags = nltk.pos_tag(words)            strtags = ["/".join((wt[0], wt[1])) for wt in tags]            out.write(" ".join(strtags) + " ")    with open(fname, "r") as fl:        line = fl.readline()  # each file is one line    wordandtag = line.strip().split()    try:        tags = [wt.split("/")[1] for wt in wordandtag]    except:        print(f"In 'POS_features': {wordandtag}")    counts = collections.Counter(tags)    for pt in pos_tags:        try:            feature_dictionary[pt] = str(float(counts[pt]) / len(tags))        except:            feature_dictionary[pt] = str(0)def vadersent(text, feature_dictionary):    analyzer = SentimentIntensityAnalyzer()    vs = analyzer.polarity_scores(text)    feature_dictionary['vad_neg'] = vs['neg']    feature_dictionary['vad_neu'] = vs['neu']    feature_dictionary['vad_pos'] = vs['pos']def readability(text, feature_dictionary):    rd = Readability(text)    fkg_score = rd.FleschKincaidGradeLevel()    SMOG = rd.SMOGIndex()    feature_dictionary['FKE'] = fkg_score    feature_dictionary['SMOG'] = SMOGdef wordlen_and_stop(text, feature_dictionary):    words = word_tokenize(text)    WC = len(words)    stopwords_in_text = [s for s in words if s in stopwords]    percent_sws = float(len(stopwords_in_text)) / len(words)    lengths = [len(w) for w in words if w not in stopwords]    if len(lengths) == 0:        word_len_avg = 3    else:        word_len_avg = float(sum(lengths)) / len(lengths)    feature_dictionary['stop'] = percent_sws    feature_dictionary['wordlen'] = word_len_avg    feature_dictionary['WC'] = WCdef stuff_LIWC_leftout(pid, text, feature_dictionary):    puncs = set(string.punctuation)    tokens = word_tokenize(text)    quotes = tokens.count("\"") + tokens.count('``') + tokens.count("''")    Exclaim = tokens.count("!")    AllPunc = 0    for p in puncs:        AllPunc += tokens.count(p)    words_upper = 0    for w in tokens:        if w.isupper():            words_upper += 1    try:        allcaps = float(words_upper) / len(tokens)    except:        allcaps = 0    feature_dictionary['quotes'] = (float(quotes) / len(tokens)) * 100    feature_dictionary['Exclaim'] = (float(Exclaim) / len(tokens)) * 100    feature_dictionary['AllPunc'] = float(AllPunc) / len(tokens) * 100    feature_dictionary['allcaps'] = allcapsdef subjectivity(loaded_model, count_vect, tfidf_transformer, text, feature_dictionary):    X_new_counts = count_vect.transform([text])    X_new_tfidf = tfidf_transformer.transform(X_new_counts)    result = loaded_model.predict_proba(X_new_tfidf)    prob_obj = result[0][0]    prob_subj = result[0][1]    feature_dictionary["NB_pobj"] = prob_obj    feature_dictionary["NB_psubj"] = prob_subjdef load_LIWC_dictionaries(filepath=utility_data_path):    cat_dict = {}    stem_dict = {}    counts_dict = {}    with open(os.path.join(filepath, "LIWC2007_English100131.dic")) as raw:        raw.readline()        for line in raw:            if line.strip() == "%":                break            line = line.strip().split()            cat_dict[line[0]] = line[1]            counts_dict[line[0]] = 0        for line in raw:            line = line.strip().split()            stem_dict[line[0]] = [l.replace("*", "") for l in line[1:]]    return cat_dict, stem_dict, counts_dictdef LIWC(text, cat_dict, stem_dict, counts_dict, feature_dictionary):    for key in counts_dict:        counts_dict[key] = 0    tokens = word_tokenize(text)    WC = len(tokens)    stemmer = PorterStemmer()    stemed_tokens = [stemmer.stem(t) for t in tokens]    # count and percentage    for stem in stem_dict:        count = stemed_tokens.count(stem.replace("*", ""))        if count > 0:            for cat in stem_dict[stem]:                counts_dict[cat] += count    counts_norm = [float(counts_dict[cat]) / WC * 100 for cat in counts_dict]    cats = [cat_dict[cat] for cat in cat_dict]    i = 0    for cat in cats:        feature_dictionary[cat] = counts_norm[i]        i += 1# Other Functions <-----------------------------------------------------------------------------------------------def fix(text):    try:        text = text.decode("ascii", "ignore")    except:        # t = [unicodedata.normalize('NFKD', unicode(q)).encode('ascii', 'ignore') for q in text]        t = [unicodedata.normalize('NFKD', q).encode('ascii', 'ignore') for q in text]        text = ''.join([val.decode('utf-8', 'ignore') for val in t]).strip()    return textdef whatsbeendon(filename):    pids = []    try:        with open(filename) as data:            pids = [line.strip().split(",")[0] for line in data]        return set(pids)    except:        return set(pids)def make_str(seq):    strseq = [str(s) for s in seq]    return strseqdef runInParallel(*funs):    proc = []    for fun in funs:        p = Process(target=fun)        p.start()        proc.append(p)    for p in proc:        p.join()